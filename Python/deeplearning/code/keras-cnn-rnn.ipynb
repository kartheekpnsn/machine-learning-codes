{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Sample Codes for CNN and RNN\n",
    "![alt text](https://blog.keras.io/img/keras-tensorflow-logo.jpg \"Keras with Tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1) MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b100133240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # for data preparation\n",
    "from keras.utils import np_utils # for one hot encoding\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a 3-dimensional array of instance, image width and image height. For a multi-layer perceptron model we must reduce the images down into a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten 28 x 28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "# It is almost always a good idea to perform some scaling of input values when using neural network models\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model with Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim = num_pixels, activation = 'relu', kernel_initializer = 'normal')) # one hidden layer with 784 neurons\n",
    "    model.add(Dense(num_classes, activation = 'softmax', kernel_initializer = 'normal')) # output layer with 10 classes\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the model over 10 epochs with updates every 200 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.2964 - acc: 0.9177 - val_loss: 0.1284 - val_acc: 0.9618\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.1045 - acc: 0.9680 - val_loss: 0.0889 - val_acc: 0.9705\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0644 - acc: 0.9802 - val_loss: 0.0825 - val_acc: 0.9766\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0458 - acc: 0.9859 - val_loss: 0.0686 - val_acc: 0.9796\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0322 - acc: 0.9903 - val_loss: 0.0700 - val_acc: 0.9785\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0217 - acc: 0.9941 - val_loss: 0.0640 - val_acc: 0.9801\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0682 - val_acc: 0.9799\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0126 - acc: 0.9962 - val_loss: 0.0673 - val_acc: 0.9805\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0124 - acc: 0.9960 - val_loss: 0.0709 - val_acc: 0.9815\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0083 - acc: 0.9973 - val_loss: 0.0705 - val_acc: 0.9804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b10129db70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 10, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0705060733277\n",
      "Accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Loss: ' + str(scores[0])) # loss\n",
    "print('Accuracy: ' + str(scores[1])) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height].\n",
    "\n",
    "In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In the case of MNIST where the pixel values are gray scale, the pixel dimension is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**: \n",
    " 1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height].\n",
    " 2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.\n",
    " 3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    " 4. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    " 5. Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    " 6. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape = (1, 28, 28), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "76s - loss: 0.2447 - acc: 0.9282 - val_loss: 0.0813 - val_acc: 0.9753\n",
      "Epoch 2/10\n",
      "70s - loss: 0.0727 - acc: 0.9786 - val_loss: 0.0493 - val_acc: 0.9849\n",
      "Epoch 3/10\n",
      "72s - loss: 0.0527 - acc: 0.9839 - val_loss: 0.0409 - val_acc: 0.9869\n",
      "Epoch 4/10\n",
      "71s - loss: 0.0404 - acc: 0.9876 - val_loss: 0.0405 - val_acc: 0.9878\n",
      "Epoch 5/10\n",
      "71s - loss: 0.0321 - acc: 0.9902 - val_loss: 0.0349 - val_acc: 0.9885\n",
      "Epoch 6/10\n",
      "71s - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0342 - val_acc: 0.9885\n",
      "Epoch 7/10\n",
      "70s - loss: 0.0229 - acc: 0.9925 - val_loss: 0.0373 - val_acc: 0.9877\n",
      "Epoch 8/10\n",
      "72s - loss: 0.0199 - acc: 0.9938 - val_loss: 0.0339 - val_acc: 0.9888\n",
      "Epoch 9/10\n",
      "71s - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0489 - val_acc: 0.9850\n",
      "Epoch 10/10\n",
      "73s - loss: 0.0150 - acc: 0.9952 - val_loss: 0.0378 - val_acc: 0.9881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b108f40ef0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we define a large CNN architecture with additional convolutional, max pooling layers and fully connected layers. The network topology can be summarized as follows.\n",
    "\n",
    "1. Convolutional layer with 30 feature maps of size 5×5.\n",
    "2. Pooling layer taking the max over 2*2 patches.\n",
    "3. Convolutional layer with 15 feature maps of size 3×3.\n",
    "4. Pooling layer taking the max over 2*2 patches.\n",
    "5. Dropout layer with a probability of 20%.\n",
    "6. Flatten layer.\n",
    "7. Fully connected layer with 128 neurons and rectifier activation.\n",
    "8. Fully connected layer with 50 neurons and rectifier activation.\n",
    "9. Output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the larger model\n",
    "def larger_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, (5, 5), input_shape = (1, 28, 28), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 81s - loss: 0.3674 - acc: 0.8884 - val_loss: 0.0826 - val_acc: 0.9744\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 80s - loss: 0.0952 - acc: 0.9706 - val_loss: 0.0557 - val_acc: 0.9811\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 79s - loss: 0.0698 - acc: 0.9788 - val_loss: 0.0411 - val_acc: 0.9865\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 79s - loss: 0.0582 - acc: 0.9822 - val_loss: 0.0357 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 80s - loss: 0.0473 - acc: 0.9853 - val_loss: 0.0358 - val_acc: 0.9875\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 78s - loss: 0.0434 - acc: 0.9865 - val_loss: 0.0308 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 82s - loss: 0.0388 - acc: 0.9881 - val_loss: 0.0312 - val_acc: 0.9887\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 79s - loss: 0.0343 - acc: 0.9896 - val_loss: 0.0262 - val_acc: 0.9916\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 80s - loss: 0.0324 - acc: 0.9895 - val_loss: 0.0235 - val_acc: 0.9915\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 80s - loss: 0.0301 - acc: 0.9902 - val_loss: 0.0294 - val_acc: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b11e0b5f60>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02938600545321824, 0.99099999999999999]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # own data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_test(image_path, plot = True):\n",
    "    image = cv2.imread(image_path)\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (28, 28), 0, 0, cv2.INTER_LINEAR)\n",
    "    image = image.astype(np.float32)\n",
    "    image = np.multiply(image, 1.0 / 255.0)\n",
    "    image = abs(image - 1)\n",
    "    image = image[:, :, 1]\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    image = image.reshape(1, 1, 28, 28).astype('float32')\n",
    "    return(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b10837bcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = generate_test('..//data//mnist//6_written.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45867258"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][np.argmax(pred)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
